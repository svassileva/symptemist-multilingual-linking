{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696541407378,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "TRv18xWQbOhr"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "\n",
    "run_name = \"clinical-xlm-roberta-crf-strict\"\n",
    "\n",
    "model_checkpoint = \"checkpoint_path\"\n",
    "num_labels = 3\n",
    "\n",
    "data_path = \"path_to_dataset_split_into_sentences\"\n",
    "original_texts_path = \"path_to_original_symptemist_texts\"\n",
    "offsets_path = \"path_to_sentence_offsets\"\n",
    "output_root_path = f\"output_path\"\n",
    "output_per_file_path = f\"{output_root_path}/per_file\" # inferred labels by file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19954,
     "status": "ok",
     "timestamp": 1696541427328,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "QXBprkFDZszi",
    "outputId": "bb940fb4-e182-4b5d-88d7-c8717dcc8b54"
   },
   "outputs": [],
   "source": [
    "# If running in Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39793,
     "status": "ok",
     "timestamp": 1696541467116,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "Fn2VOMHVZdO6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14626,
     "status": "ok",
     "timestamp": 1696541481732,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "1RXA3_zObUkl",
    "outputId": "fecab20e-3ada-4e38-e6c2-82b4182af2b0"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, TokenClassificationPipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwqM6s-LXeeh"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1696541481732,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "nbHFaDKdlDkH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchcrf import CRF\n",
    "from transformers import XLMRobertaPreTrainedModel, XLMRobertaModel, TrainingArguments, Trainer\n",
    "from transformers.modeling_outputs import  TokenClassifierOutput\n",
    "\n",
    "class XLMRobertaWithCRF(XLMRobertaPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = XLMRobertaModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(num_tags=self.num_labels, batch_first=True)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            mask = torch.where(labels == -100, torch.tensor(0), torch.tensor(1))\n",
    "            mask[:, 0] = 1 # shameless hack\n",
    "            mask = mask.bool()\n",
    "            hacked_labels = torch.where(labels == -100, torch.tensor(0), labels) # another one, thanks https://github.com/kmkurn/pytorch-crf/issues/41\n",
    "            log_likelihood, tags = self.crf(logits, hacked_labels, mask=mask), self.crf.decode(logits)\n",
    "            loss = 0 - log_likelihood\n",
    "        else:\n",
    "            tags = self.crf.decode(logits)\n",
    "        tags = torch.Tensor(tags).int()\n",
    "\n",
    "        output = (tags,) + outputs[2:]\n",
    "        # print(\"Returning: \", ((loss,) + output) if loss is not None else output)\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31844,
     "status": "ok",
     "timestamp": 1696541513564,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "GcEUdQZAlOgv"
   },
   "outputs": [],
   "source": [
    "model = XLMRobertaWithCRF.from_pretrained(model_checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1696541513565,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "brw-NgekyC7P"
   },
   "outputs": [],
   "source": [
    "import types\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "from transformers.pipelines.base import ChunkPipeline\n",
    "from transformers.pipelines.token_classification import TokenClassificationArgumentHandler, AggregationStrategy\n",
    "from transformers.models.bert.tokenization_bert import BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1696541513565,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "UzsRdA2xx5ni"
   },
   "outputs": [],
   "source": [
    "# Having a custom model means we cannot use the regular NER pipeline, so we will modify it\n",
    "class CustomTokenClassificationPipeline(ChunkPipeline):\n",
    "    \"\"\"\n",
    "    Named Entity Recognition pipeline using any `ModelForTokenClassification`. See the [named entity recognition\n",
    "    examples](../task_summary#named-entity-recognition) for more information.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import pipeline\n",
    "\n",
    "    >>> token_classifier = pipeline(model=\"Jean-Baptiste/camembert-ner\", aggregation_strategy=\"simple\")\n",
    "    >>> sentence = \"Je m'appelle jean-baptiste et je vis à montréal\"\n",
    "    >>> tokens = token_classifier(sentence)\n",
    "    >>> tokens\n",
    "    [{'entity_group': 'PER', 'score': 0.9931, 'word': 'jean-baptiste', 'start': 12, 'end': 26}, {'entity_group': 'LOC', 'score': 0.998, 'word': 'montréal', 'start': 38, 'end': 47}]\n",
    "\n",
    "    >>> token = tokens[0]\n",
    "    >>> # Start and end provide an easy way to highlight words in the original text.\n",
    "    >>> sentence[token[\"start\"] : token[\"end\"]]\n",
    "    ' jean-baptiste'\n",
    "\n",
    "    >>> # Some models use the same idea to do part of speech.\n",
    "    >>> syntaxer = pipeline(model=\"vblagoje/bert-english-uncased-finetuned-pos\", aggregation_strategy=\"simple\")\n",
    "    >>> syntaxer(\"My name is Sarah and I live in London\")\n",
    "    [{'entity_group': 'PRON', 'score': 0.999, 'word': 'my', 'start': 0, 'end': 2}, {'entity_group': 'NOUN', 'score': 0.997, 'word': 'name', 'start': 3, 'end': 7}, {'entity_group': 'AUX', 'score': 0.994, 'word': 'is', 'start': 8, 'end': 10}, {'entity_group': 'PROPN', 'score': 0.999, 'word': 'sarah', 'start': 11, 'end': 16}, {'entity_group': 'CCONJ', 'score': 0.999, 'word': 'and', 'start': 17, 'end': 20}, {'entity_group': 'PRON', 'score': 0.999, 'word': 'i', 'start': 21, 'end': 22}, {'entity_group': 'VERB', 'score': 0.998, 'word': 'live', 'start': 23, 'end': 27}, {'entity_group': 'ADP', 'score': 0.999, 'word': 'in', 'start': 28, 'end': 30}, {'entity_group': 'PROPN', 'score': 0.999, 'word': 'london', 'start': 31, 'end': 37}]\n",
    "    ```\n",
    "\n",
    "    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
    "\n",
    "    This token recognition pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
    "    `\"ner\"` (for predicting the classes of tokens in a sequence: person, organisation, location or miscellaneous).\n",
    "\n",
    "    The models that this pipeline can use are models that have been fine-tuned on a token classification task. See the\n",
    "    up-to-date list of available models on\n",
    "    [huggingface.co/models](https://huggingface.co/models?filter=token-classification).\n",
    "    \"\"\"\n",
    "\n",
    "    default_input_names = \"sequences\"\n",
    "\n",
    "    def __init__(self, args_parser=TokenClassificationArgumentHandler(), *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "        self._args_parser = args_parser\n",
    "\n",
    "    def _sanitize_parameters(\n",
    "        self,\n",
    "        ignore_labels=None,\n",
    "        grouped_entities: Optional[bool] = None,\n",
    "        ignore_subwords: Optional[bool] = None,\n",
    "        aggregation_strategy: Optional[AggregationStrategy] = None,\n",
    "        offset_mapping: Optional[List[Tuple[int, int]]] = None,\n",
    "        stride: Optional[int] = None,\n",
    "    ):\n",
    "        preprocess_params = {}\n",
    "        if offset_mapping is not None:\n",
    "            preprocess_params[\"offset_mapping\"] = offset_mapping\n",
    "\n",
    "        postprocess_params = {}\n",
    "        if grouped_entities is not None or ignore_subwords is not None:\n",
    "            if grouped_entities and ignore_subwords:\n",
    "                aggregation_strategy = AggregationStrategy.FIRST\n",
    "            elif grouped_entities and not ignore_subwords:\n",
    "                aggregation_strategy = AggregationStrategy.SIMPLE\n",
    "            else:\n",
    "                aggregation_strategy = AggregationStrategy.NONE\n",
    "\n",
    "            if grouped_entities is not None:\n",
    "                warnings.warn(\n",
    "                    \"`grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to\"\n",
    "                    f' `aggregation_strategy=\"{aggregation_strategy}\"` instead.'\n",
    "                )\n",
    "            if ignore_subwords is not None:\n",
    "                warnings.warn(\n",
    "                    \"`ignore_subwords` is deprecated and will be removed in version v5.0.0, defaulted to\"\n",
    "                    f' `aggregation_strategy=\"{aggregation_strategy}\"` instead.'\n",
    "                )\n",
    "\n",
    "        if aggregation_strategy is not None:\n",
    "            if isinstance(aggregation_strategy, str):\n",
    "                aggregation_strategy = AggregationStrategy[aggregation_strategy.upper()]\n",
    "            if (\n",
    "                aggregation_strategy\n",
    "                in {AggregationStrategy.FIRST, AggregationStrategy.MAX, AggregationStrategy.AVERAGE}\n",
    "                and not self.tokenizer.is_fast\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Slow tokenizers cannot handle subwords. Please set the `aggregation_strategy` option\"\n",
    "                    ' to `\"simple\"` or use a fast tokenizer.'\n",
    "                )\n",
    "            postprocess_params[\"aggregation_strategy\"] = aggregation_strategy\n",
    "        if ignore_labels is not None:\n",
    "            postprocess_params[\"ignore_labels\"] = ignore_labels\n",
    "        if stride is not None:\n",
    "            if stride >= self.tokenizer.model_max_length:\n",
    "                raise ValueError(\n",
    "                    \"`stride` must be less than `tokenizer.model_max_length` (or even lower if the tokenizer adds special tokens)\"\n",
    "                )\n",
    "            if aggregation_strategy == AggregationStrategy.NONE:\n",
    "                raise ValueError(\n",
    "                    \"`stride` was provided to process all the text but `aggregation_strategy=\"\n",
    "                    f'\"{aggregation_strategy}\"`, please select another one instead.'\n",
    "                )\n",
    "            else:\n",
    "                if self.tokenizer.is_fast:\n",
    "                    tokenizer_params = {\n",
    "                        \"return_overflowing_tokens\": True,\n",
    "                        \"padding\": True,\n",
    "                        \"stride\": stride,\n",
    "                    }\n",
    "                    preprocess_params[\"tokenizer_params\"] = tokenizer_params\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"`stride` was provided to process all the text but you're using a slow tokenizer.\"\n",
    "                        \" Please use a fast tokenizer.\"\n",
    "                    )\n",
    "        return preprocess_params, {}, postprocess_params\n",
    "\n",
    "    def __call__(self, inputs: Union[str, List[str]], **kwargs):\n",
    "        \"\"\"\n",
    "        Classify each token of the text(s) given as inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs (`str` or `List[str]`):\n",
    "                One or several texts (or one list of texts) for token classification.\n",
    "\n",
    "        Return:\n",
    "            A list or a list of list of `dict`: Each result comes as a list of dictionaries (one for each token in the\n",
    "            corresponding input, or each entity if this pipeline was instantiated with an aggregation_strategy) with\n",
    "            the following keys:\n",
    "\n",
    "            - **word** (`str`) -- The token/word classified. This is obtained by decoding the selected tokens. If you\n",
    "              want to have the exact string in the original sentence, use `start` and `end`.\n",
    "            - **score** (`float`) -- The corresponding probability for `entity`.\n",
    "            - **entity** (`str`) -- The entity predicted for that token/word (it is named *entity_group* when\n",
    "              *aggregation_strategy* is not `\"none\"`.\n",
    "            - **index** (`int`, only present when `aggregation_strategy=\"none\"`) -- The index of the corresponding\n",
    "              token in the sentence.\n",
    "            - **start** (`int`, *optional*) -- The index of the start of the corresponding entity in the sentence. Only\n",
    "              exists if the offsets are available within the tokenizer\n",
    "            - **end** (`int`, *optional*) -- The index of the end of the corresponding entity in the sentence. Only\n",
    "              exists if the offsets are available within the tokenizer\n",
    "        \"\"\"\n",
    "\n",
    "        _inputs, offset_mapping = self._args_parser(inputs, **kwargs)\n",
    "        if offset_mapping:\n",
    "            kwargs[\"offset_mapping\"] = offset_mapping\n",
    "\n",
    "        return super().__call__(inputs, **kwargs)\n",
    "\n",
    "    def preprocess(self, sentence, offset_mapping=None, **preprocess_params):\n",
    "        tokenizer_params = preprocess_params.pop(\"tokenizer_params\", {})\n",
    "        truncation = True if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else False\n",
    "        inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=self.framework,\n",
    "            truncation=truncation,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_offsets_mapping=self.tokenizer.is_fast,\n",
    "            **tokenizer_params,\n",
    "        )\n",
    "        inputs.pop(\"overflow_to_sample_mapping\", None)\n",
    "        num_chunks = len(inputs[\"input_ids\"])\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            model_inputs = {k: v[i].unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "            if offset_mapping is not None:\n",
    "                model_inputs[\"offset_mapping\"] = offset_mapping\n",
    "            model_inputs[\"sentence\"] = sentence if i == 0 else None\n",
    "            model_inputs[\"is_last\"] = i == num_chunks - 1\n",
    "\n",
    "            yield model_inputs\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        # Forward\n",
    "        special_tokens_mask = model_inputs.pop(\"special_tokens_mask\")\n",
    "        offset_mapping = model_inputs.pop(\"offset_mapping\", None)\n",
    "        sentence = model_inputs.pop(\"sentence\")\n",
    "        is_last = model_inputs.pop(\"is_last\")\n",
    "        if self.framework == \"tf\":\n",
    "            logits = self.model(**model_inputs)[0]\n",
    "        else:\n",
    "            output = self.model(**model_inputs)\n",
    "            # print(\"Model output in _forward: \", output)\n",
    "            logits = output[\"logits\"] if isinstance(output, dict) else output[0]\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"special_tokens_mask\": special_tokens_mask,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"sentence\": sentence,\n",
    "            \"is_last\": is_last,\n",
    "            **model_inputs,\n",
    "        }\n",
    "\n",
    "    def postprocess(self, all_outputs, aggregation_strategy=AggregationStrategy.NONE, ignore_labels=None):\n",
    "        # print(\"postprocess all_outputs: \", all_outputs)\n",
    "        if ignore_labels is None:\n",
    "            ignore_labels = [\"O\"]\n",
    "        all_entities = []\n",
    "        for model_outputs in all_outputs:\n",
    "            logits = model_outputs[\"logits\"][0].numpy()\n",
    "            sentence = all_outputs[0][\"sentence\"]\n",
    "            input_ids = model_outputs[\"input_ids\"][0]\n",
    "            offset_mapping = (\n",
    "                model_outputs[\"offset_mapping\"][0] if model_outputs[\"offset_mapping\"] is not None else None\n",
    "            )\n",
    "            special_tokens_mask = model_outputs[\"special_tokens_mask\"][0].numpy()\n",
    "\n",
    "            # maxes = np.max(logits, axis=-1, keepdims=True)\n",
    "            # shifted_exp = np.exp(logits - maxes)\n",
    "            # scores = shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)\n",
    "            scores = logits\n",
    "            # print(\"postprocess scores: \", scores)\n",
    "\n",
    "            if self.framework == \"tf\":\n",
    "                input_ids = input_ids.numpy()\n",
    "                offset_mapping = offset_mapping.numpy() if offset_mapping is not None else None\n",
    "\n",
    "            pre_entities = self.gather_pre_entities(\n",
    "                sentence, input_ids, scores, offset_mapping, special_tokens_mask, aggregation_strategy\n",
    "            )\n",
    "\n",
    "            # print(\"postprocess pre_entities: \", pre_entities)\n",
    "\n",
    "            grouped_entities = self.aggregate(pre_entities, aggregation_strategy)\n",
    "\n",
    "            # print(\"postprocess grouped_entities: \", grouped_entities)\n",
    "\n",
    "            # Filter anything that is in self.ignore_labels\n",
    "            entities = [\n",
    "                entity\n",
    "                for entity in grouped_entities\n",
    "                if entity.get(\"entity\", None) not in ignore_labels\n",
    "                and entity.get(\"entity_group\", None) not in ignore_labels\n",
    "            ]\n",
    "            all_entities.extend(entities)\n",
    "        num_chunks = len(all_outputs)\n",
    "        if num_chunks > 1:\n",
    "            all_entities = self.aggregate_overlapping_entities(all_entities)\n",
    "        return all_entities\n",
    "\n",
    "    def aggregate_overlapping_entities(self, entities):\n",
    "        if len(entities) == 0:\n",
    "            return entities\n",
    "        entities = sorted(entities, key=lambda x: x[\"start\"])\n",
    "        aggregated_entities = []\n",
    "        previous_entity = entities[0]\n",
    "        for entity in entities:\n",
    "            if previous_entity[\"start\"] <= entity[\"start\"] < previous_entity[\"end\"]:\n",
    "                current_length = entity[\"end\"] - entity[\"start\"]\n",
    "                previous_length = previous_entity[\"end\"] - previous_entity[\"start\"]\n",
    "                if current_length > previous_length:\n",
    "                    previous_entity = entity\n",
    "                elif current_length == previous_length and entity[\"score\"] > previous_entity[\"score\"]:\n",
    "                    previous_entity = entity\n",
    "            else:\n",
    "                aggregated_entities.append(previous_entity)\n",
    "                previous_entity = entity\n",
    "        aggregated_entities.append(previous_entity)\n",
    "        return aggregated_entities\n",
    "\n",
    "    def gather_pre_entities(\n",
    "        self,\n",
    "        sentence: str,\n",
    "        input_ids: np.ndarray,\n",
    "        scores: np.ndarray,\n",
    "        offset_mapping: Optional[List[Tuple[int, int]]],\n",
    "        special_tokens_mask: np.ndarray,\n",
    "        aggregation_strategy: AggregationStrategy,\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\"\n",
    "        pre_entities = []\n",
    "        for idx, token_scores in enumerate(scores):\n",
    "            # Filter special_tokens\n",
    "            if special_tokens_mask[idx]:\n",
    "                continue\n",
    "\n",
    "            word = self.tokenizer.convert_ids_to_tokens(int(input_ids[idx]))\n",
    "            if offset_mapping is not None:\n",
    "                start_ind, end_ind = offset_mapping[idx]\n",
    "                if not isinstance(start_ind, int):\n",
    "                    if self.framework == \"pt\":\n",
    "                        start_ind = start_ind.item()\n",
    "                        end_ind = end_ind.item()\n",
    "                word_ref = sentence[start_ind:end_ind]\n",
    "                if getattr(self.tokenizer, \"_tokenizer\", None) and getattr(\n",
    "                    self.tokenizer._tokenizer.model, \"continuing_subword_prefix\", None\n",
    "                ):\n",
    "                    # This is a BPE, word aware tokenizer, there is a correct way\n",
    "                    # to fuse tokens\n",
    "                    is_subword = len(word) != len(word_ref)\n",
    "                else:\n",
    "                    # This is a fallback heuristic. This will fail most likely on any kind of text + punctuation mixtures that will be considered \"words\". Non word aware models cannot do better than this unfortunately.\n",
    "                    if aggregation_strategy in {\n",
    "                        AggregationStrategy.FIRST,\n",
    "                        AggregationStrategy.AVERAGE,\n",
    "                        AggregationStrategy.MAX,\n",
    "                    }:\n",
    "                        warnings.warn(\n",
    "                            \"Tokenizer does not support real words, using fallback heuristic\",\n",
    "                            UserWarning,\n",
    "                        )\n",
    "                    is_subword = start_ind > 0 and \" \" not in sentence[start_ind - 1 : start_ind + 1]\n",
    "\n",
    "                if int(input_ids[idx]) == self.tokenizer.unk_token_id:\n",
    "                    word = word_ref\n",
    "                    is_subword = False\n",
    "            else:\n",
    "                start_ind = None\n",
    "                end_ind = None\n",
    "                is_subword = False\n",
    "\n",
    "            pre_entity = {\n",
    "                \"word\": word,\n",
    "                \"scores\": token_scores,\n",
    "                \"start\": start_ind,\n",
    "                \"end\": end_ind,\n",
    "                \"index\": idx,\n",
    "                \"is_subword\": is_subword,\n",
    "            }\n",
    "            pre_entities.append(pre_entity)\n",
    "        return pre_entities\n",
    "\n",
    "    def aggregate(self, pre_entities: List[dict], aggregation_strategy: AggregationStrategy) -> List[dict]:\n",
    "        if aggregation_strategy in {AggregationStrategy.NONE, AggregationStrategy.SIMPLE}:\n",
    "            entities = []\n",
    "            for pre_entity in pre_entities:\n",
    "                # entity_idx = pre_entity[\"scores\"].argmax()\n",
    "\n",
    "                # print(\"pre_entity scores in aggregate: \", pre_entity[\"scores\"])\n",
    "                # print(\"entity_idx in aggregate: \", entity_idx)\n",
    "\n",
    "                # score = pre_entity[\"scores\"][entity_idx]\n",
    "                entity = {\n",
    "                    \"entity\": self.model.config.id2label[pre_entity[\"scores\"]],\n",
    "                    \"score\": pre_entity[\"scores\"],\n",
    "                    # \"score\": score,\n",
    "                    \"index\": pre_entity[\"index\"],\n",
    "                    \"word\": pre_entity[\"word\"],\n",
    "                    \"start\": pre_entity[\"start\"],\n",
    "                    \"end\": pre_entity[\"end\"],\n",
    "                }\n",
    "                entities.append(entity)\n",
    "        else:\n",
    "            entities = self.aggregate_words(pre_entities, aggregation_strategy)\n",
    "\n",
    "        if aggregation_strategy == AggregationStrategy.NONE:\n",
    "            return entities\n",
    "        return self.group_entities(entities)\n",
    "\n",
    "    def aggregate_word(self, entities: List[dict], aggregation_strategy: AggregationStrategy) -> dict:\n",
    "        word = self.tokenizer.convert_tokens_to_string([entity[\"word\"] for entity in entities])\n",
    "        if aggregation_strategy == AggregationStrategy.FIRST:\n",
    "            scores = entities[0][\"scores\"]\n",
    "            idx = scores.argmax()\n",
    "            score = scores[idx]\n",
    "            entity = self.model.config.id2label[idx]\n",
    "        elif aggregation_strategy == AggregationStrategy.MAX:\n",
    "            max_entity = max(entities, key=lambda entity: entity[\"scores\"].max())\n",
    "            scores = max_entity[\"scores\"]\n",
    "            idx = scores.argmax()\n",
    "            score = scores[idx]\n",
    "            entity = self.model.config.id2label[idx]\n",
    "        elif aggregation_strategy == AggregationStrategy.AVERAGE:\n",
    "            scores = np.stack([entity[\"scores\"] for entity in entities])\n",
    "            average_scores = np.nanmean(scores, axis=0)\n",
    "            entity_idx = average_scores.argmax()\n",
    "            entity = self.model.config.id2label[entity_idx]\n",
    "            score = average_scores[entity_idx]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation_strategy\")\n",
    "        new_entity = {\n",
    "            \"entity\": entity,\n",
    "            \"score\": score,\n",
    "            \"word\": word,\n",
    "            \"start\": entities[0][\"start\"],\n",
    "            \"end\": entities[-1][\"end\"],\n",
    "        }\n",
    "        return new_entity\n",
    "\n",
    "    def aggregate_words(self, entities: List[dict], aggregation_strategy: AggregationStrategy) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Override tokens from a given word that disagree to force agreement on word boundaries.\n",
    "\n",
    "        Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft|\n",
    "        company| B-ENT I-ENT\n",
    "        \"\"\"\n",
    "        if aggregation_strategy in {\n",
    "            AggregationStrategy.NONE,\n",
    "            AggregationStrategy.SIMPLE,\n",
    "        }:\n",
    "            raise ValueError(\"NONE and SIMPLE strategies are invalid for word aggregation\")\n",
    "\n",
    "        word_entities = []\n",
    "        word_group = None\n",
    "        for entity in entities:\n",
    "            if word_group is None:\n",
    "                word_group = [entity]\n",
    "            elif entity[\"is_subword\"]:\n",
    "                word_group.append(entity)\n",
    "            else:\n",
    "                word_entities.append(self.aggregate_word(word_group, aggregation_strategy))\n",
    "                word_group = [entity]\n",
    "        # Last item\n",
    "        if word_group is not None:\n",
    "            word_entities.append(self.aggregate_word(word_group, aggregation_strategy))\n",
    "        return word_entities\n",
    "\n",
    "    def group_sub_entities(self, entities: List[dict]) -> dict:\n",
    "        \"\"\"\n",
    "        Group together the adjacent tokens with the same entity predicted.\n",
    "\n",
    "        Args:\n",
    "            entities (`dict`): The entities predicted by the pipeline.\n",
    "        \"\"\"\n",
    "        # Get the first entity in the entity group\n",
    "        entity = entities[0][\"entity\"].split(\"-\")[-1]\n",
    "        scores = np.nanmean([entity[\"score\"] for entity in entities])\n",
    "        tokens = [entity[\"word\"] for entity in entities]\n",
    "\n",
    "        entity_group = {\n",
    "            \"entity_group\": entity,\n",
    "            \"score\": np.mean(scores),\n",
    "            \"word\": self.tokenizer.convert_tokens_to_string(tokens),\n",
    "            \"start\": entities[0][\"start\"],\n",
    "            \"end\": entities[-1][\"end\"],\n",
    "        }\n",
    "        return entity_group\n",
    "\n",
    "    def get_tag(self, entity_name: str) -> Tuple[str, str]:\n",
    "        if entity_name.startswith(\"B-\"):\n",
    "            bi = \"B\"\n",
    "            tag = entity_name[2:]\n",
    "        elif entity_name.startswith(\"I-\"):\n",
    "            bi = \"I\"\n",
    "            tag = entity_name[2:]\n",
    "        else:\n",
    "            # It's not in B-, I- format\n",
    "            # Default to I- for continuation.\n",
    "            bi = \"I\"\n",
    "            tag = entity_name\n",
    "        return bi, tag\n",
    "\n",
    "    def group_entities(self, entities: List[dict]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Find and group together the adjacent tokens with the same entity predicted.\n",
    "\n",
    "        Args:\n",
    "            entities (`dict`): The entities predicted by the pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        entity_groups = []\n",
    "        entity_group_disagg = []\n",
    "\n",
    "        for entity in entities:\n",
    "            if not entity_group_disagg:\n",
    "                entity_group_disagg.append(entity)\n",
    "                continue\n",
    "\n",
    "            # If the current entity is similar and adjacent to the previous entity,\n",
    "            # append it to the disaggregated entity group\n",
    "            # The split is meant to account for the \"B\" and \"I\" prefixes\n",
    "            # Shouldn't merge if both entities are B-type\n",
    "            bi, tag = self.get_tag(entity[\"entity\"])\n",
    "            last_bi, last_tag = self.get_tag(entity_group_disagg[-1][\"entity\"])\n",
    "\n",
    "            if tag == last_tag and bi != \"B\":\n",
    "                # Modify subword type to be previous_type\n",
    "                entity_group_disagg.append(entity)\n",
    "            else:\n",
    "                # If the current entity is different from the previous entity\n",
    "                # aggregate the disaggregated entity group\n",
    "                entity_groups.append(self.group_sub_entities(entity_group_disagg))\n",
    "                entity_group_disagg = [entity]\n",
    "        if entity_group_disagg:\n",
    "            # it's the last entity, add it to the entity groups\n",
    "            entity_groups.append(self.group_sub_entities(entity_group_disagg))\n",
    "\n",
    "        return entity_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDMPt8RxXmS0"
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1696541513565,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "LT5_3bee2sZP"
   },
   "outputs": [],
   "source": [
    "ner_pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", stride=0, pipeline_class=CustomTokenClassificationPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1696541514116,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "_p_JXsSB2yIv",
    "outputId": "4a48a5e5-f0e7-4743-c2c3-7c5025be154c"
   },
   "outputs": [],
   "source": [
    "ner_pipe(\"Paciente varón de 35 años con tumoración en polo superior de teste derecho hallada de manera casual durante una autoexploración, motivo por el cual acude a consulta de urología donde se realiza exploración física, apreciando masa de 1cm aproximado de diámetro dependiente de epidídimo, y ecografía testicular, que se informa como lesión nodular sólida en cabeza de epidídimo derecho.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696541514116,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "rSlDbg9lXKiE"
   },
   "outputs": [],
   "source": [
    "# group annotations around a clinical procedure mention, based on the annotation label\n",
    "def group_annotations_strict(annotations):\n",
    "  groups = []\n",
    "  i = 0\n",
    "  while i < len(annotations):\n",
    "    if annotations[i]['entity_group'] == 'LABEL_0':\n",
    "      i += 1\n",
    "      continue\n",
    "\n",
    "    group = [] # for the strict strategy, a group is a B (or many Bs), followed by 1 or more Is\n",
    "    if annotations[i]['entity_group'] == 'LABEL_1':\n",
    "      group.append(annotations[i])\n",
    "      i += 1\n",
    "\n",
    "      while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_1'):\n",
    "          group.append(annotations[i])\n",
    "          i += 1\n",
    "\n",
    "      while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_2'):\n",
    "          group.append(annotations[i])\n",
    "          i += 1\n",
    "\n",
    "      groups.append(group)\n",
    "    else:\n",
    "      i+=1\n",
    "      continue\n",
    "\n",
    "  return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1696541514116,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "SsntbSAryqAn"
   },
   "outputs": [],
   "source": [
    "# merge grouped annotations to form a complete entity mention\n",
    "def merge_annotation_group_entries(annotation_group):\n",
    "  start = annotation_group[0]['start']\n",
    "  end = annotation_group[len(annotation_group) - 1]['end']\n",
    "  text = ' '.join(annotation['word'] for annotation in annotation_group)\n",
    "  return {'start': start, 'end': end, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1696541514116,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "8f574bI6qEwE"
   },
   "outputs": [],
   "source": [
    "def get_mentions(sentence):\n",
    "  annotation_groups = group_annotations_strict(ner_pipe(sentence))\n",
    "  return [merge_annotation_group_entries(group) for group in annotation_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1696541514710,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "fcvH-XVHrLL0",
    "outputId": "377d2b11-735c-4950-b27c-d5665709b522"
   },
   "outputs": [],
   "source": [
    "get_mentions(\"En la última realizada (4 años después del diagnóstico) se aprecia alteración en la morfología del polo inferior del riñón izquierdo con disminución de la cortical y calcificación abigarrada asociada en el parénquima, todo ello en relación con su diagnóstico de TBC.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1696541515247,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "8Ts2ztin-z1J"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "offsets = pd.read_csv(offsets_path)\n",
    "offsets.set_index(\"file_name\", inplace=True)\n",
    "\n",
    "def apply_offset_to_mentions(mentions, file_name):\n",
    "  for mention in mentions:\n",
    "    offset = offsets.loc[file_name][\"start_offset\"]\n",
    "    mention[\"start\"] += offset\n",
    "    mention[\"end\"] += offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696541515247,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "69WvR2wMD5F_"
   },
   "outputs": [],
   "source": [
    "def save_mentions_tsv(processed_annotations, file_name):\n",
    "  file_name = file_name.rstrip(\".txt\")\n",
    "\n",
    "  filenames = []\n",
    "  labels = []\n",
    "  start_spans = []\n",
    "  end_spans = []\n",
    "  texts = []\n",
    "  for annotation in processed_annotations:\n",
    "    filenames.append(file_name)\n",
    "    labels.append(\"SINTOMA\")\n",
    "    start_spans.append(annotation[\"start\"])\n",
    "    end_spans.append(annotation[\"end\"])\n",
    "    texts.append(annotation[\"text\"])\n",
    "\n",
    "  df = pd.DataFrame(data={\"filename\": filenames, \"label\": labels, \"start_span\": start_spans, \"end_span\": end_spans, \"text\": texts })\n",
    "  df.to_csv(f\"{output_per_file_path}/{file_name}.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tg70OH_XvA4"
   },
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29288,
     "status": "ok",
     "timestamp": 1696541544532,
     "user": {
      "displayName": "Georgi Grazhdanski",
      "userId": "14256767753397709875"
     },
     "user_tz": -180
    },
    "id": "tO9R8jtaXxLp",
    "outputId": "7c8ba7cf-1228-40d2-ad49-449ae5342175"
   },
   "outputs": [],
   "source": [
    "file_names = list(filter(lambda file_name: file_name.endswith(\".txt\"), os.listdir(data_path)))\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cn33Xe5foW6A"
   },
   "outputs": [],
   "source": [
    "already_processed_files = [file_name.strip('.tsv') for file_name in os.listdir(output_per_file_path)]\n",
    "\n",
    "for file_name in file_names:\n",
    "  print(file_name)\n",
    "  file_name_no_ext = file_name.rstrip(\".txt\")\n",
    "\n",
    "  if file_name_no_ext in already_processed_files: # this helps in case processing has been interrupted\n",
    "    continue\n",
    "\n",
    "  mentions_in_file = []\n",
    "\n",
    "  with open(f\"{data_path}/{file_name}\") as txt_file:\n",
    "    for line in txt_file.readlines():\n",
    "      if not line:\n",
    "        continue\n",
    "\n",
    "      mentions_in_file.extend(get_mentions(line))\n",
    "\n",
    "    # after processing all lines\n",
    "    apply_offset_to_mentions(mentions_in_file, file_name)\n",
    "    save_mentions_tsv(mentions_in_file, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "erZ5ROSnpWwD"
   },
   "outputs": [],
   "source": [
    "# restore the original mention text from the source report, as encoding errors may occur when merging tokens in the pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def restore_mention_text(row):\n",
    "  txt_file = f\"{original_texts_path}/{row['filename']}.txt\"\n",
    "\n",
    "  with open(txt_file, \"r\", encoding=\"utf-8\") as src_file:\n",
    "    content = src_file.read()\n",
    "    return content[row[\"start_span\"]:row[\"end_span\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPgc2P7HGwbH"
   },
   "outputs": [],
   "source": [
    "# merge data frames for the separate files into a single result df\n",
    "dfs = []\n",
    "\n",
    "for filename in os.listdir(output_per_file_path):\n",
    "  df = pd.read_csv(f\"{output_per_file_path}/{filename}\", index_col=None, header=0, sep=\"\\t\")\n",
    "  dfs.append(df)\n",
    "\n",
    "results_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "results_df[\"filename\"] = results_df.apply(lambda row: row[\"filename\"].split(\"-b-\")[0], axis=1)\n",
    "results_df[\"text\"] = results_df.apply(restore_mention_text, axis=1)\n",
    "results_df.to_csv(f\"{output_root_path}/results.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
