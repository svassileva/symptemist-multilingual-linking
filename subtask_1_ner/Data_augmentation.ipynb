{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0223c1-4b53-4473-b294-d9c1a97ab386",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_path = \"./all_gazetteer_terms_from_umls.tsv\"\n",
    "train_split_path = \"path_to_dataset_split_into_sentences\"\n",
    "output_path = \"output_path\"\n",
    "gold_annotations_with_snomed_code_path = \"./symptemist_tsv_train_subtask2.tsv\" \n",
    "gazetteer_path = \"./symptemist_gazetter_snomed_ES_v2.tsv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08a7294-add7-491f-ba61-087370890b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bbd169a-6bad-480c-b17d-0b7f18f8cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = pd.read_csv(synonyms_path, sep=\"\\t\", encoding=\"utf8\")\n",
    "synonyms[\"snomed_code\"] = synonyms[\"snomed_code\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af02cbb-42bc-460e-8776-5dc00187d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_annotations = pd.read_csv(gold_annotations_with_snomed_code_path, sep=\"\\t\", encoding=\"utf8\")\n",
    "gold_annotations[\"code\"] = gold_annotations[\"code\"].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31aeba4-be84-43b3-aa75-a0fa3e90cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty_file(file_path):  \n",
    "    return os.path.isfile(file_path) and os.path.getsize(file_path) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916d335a-9ea8-48c5-8bd3-b29fd8415ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_snomed_code(term, source_report_file_name): \n",
    "    filtered = gold_annotations[(gold_annotations[\"filename\"] == source_report_file_name) & (gold_annotations[\"text\"] == term)]\n",
    "    if filtered.empty:\n",
    "        return None\n",
    "\n",
    "    return filtered[\"code\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7412e8fa-40e9-4fa7-8f25-28204b9913df",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_without_synonyms = []\n",
    "def get_synonyms(term, source_report_file_name):\n",
    "    code = get_term_snomed_code(term, source_report_file_name)\n",
    "    \n",
    "    if code == \"NO_CODE\" or code is None:\n",
    "        return []\n",
    "        \n",
    "    filtered = synonyms[synonyms[\"snomed_code\"] == code]\n",
    "    \n",
    "    if filtered.empty: \n",
    "        codes_without_synonyms.append(code)\n",
    "        return []\n",
    "        \n",
    "    return filtered[\"term\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b25e48b-75df-4de8-9a97-9297e510fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_string_at_position(original_string, insertion, position):\n",
    "    # slice the original string into two parts: before and after the position\n",
    "    before = original_string[:position]\n",
    "    after = original_string[position:]    \n",
    "    return before + insertion + after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127bbc99-0a08-439e-8dee-38d9eef8046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_pos(row, src_text):\n",
    "    return src_text.find(row[\"text\"])\n",
    "\n",
    "def get_end_pos(row):\n",
    "    return row[\"start_pos\"] + len(row[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90863a9-c935-462e-8726-f8b7d28827ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "unchanged_files = []\n",
    "\n",
    "all_annotations = []\n",
    "for file_name in os.listdir(train_split_path):\n",
    "    if file_name.endswith(\".ann\"):\n",
    "        continue\n",
    "        \n",
    "    file_name_no_ext = file_name.rstrip(\".txt\")\n",
    "    source_report_file_name = file_name_no_ext.split(\"-b-\")[0]\n",
    "    annotations_file_path = f\"{train_split_path}/{file_name_no_ext}.ann\"\n",
    "    augmentation_file_name_no_ext = f\"a_{file_name_no_ext}\"\n",
    "\n",
    "    if is_empty_file(annotations_file_path):\n",
    "        continue\n",
    "    \n",
    "    annotations = pd.read_csv(annotations_file_path, sep='\\t', names=['ann_type', 'entity_type', 'text'], encoding='utf-8')\n",
    "    annotations['start_pos'] = annotations['entity_type'].transform(lambda v: int(v.split()[1]))\n",
    "    annotations['end_pos'] = annotations['entity_type'].transform(lambda v: int(v.split()[2]))\n",
    "    annotations['entity_type'] = annotations['entity_type'].transform(lambda v: v.split()[0])\n",
    "    annotations['synonyms'] = annotations.apply(lambda row: get_synonyms(row[\"text\"], source_report_file_name), axis=1)\n",
    "    annotations['has_multiple_synonyms'] = annotations['synonyms'].transform(lambda v: len(v) > 1)\n",
    "    \n",
    "    candidate_annotations = annotations[annotations['has_multiple_synonyms'] == True] # just to be explicit\n",
    "    \n",
    "    if candidate_annotations.empty:\n",
    "        unchanged_files.append(file_name_no_ext)\n",
    "        continue\n",
    "    \n",
    "    with open(f\"{train_split_path}/{file_name}\", \"r\", encoding=\"utf8\") as og_file:\n",
    "        report_text = og_file.read()\n",
    "\n",
    "    annotation_to_replace = candidate_annotations.sample(1) # pick a random annotation to replace, this returns a data frame\n",
    "    replacement_term = random.choice(annotation_to_replace.iloc[0][\"synonyms\"]) # pick a random synonym of the annotation term\n",
    "    report_text = report_text.replace(annotation_to_replace.iloc[0][\"text\"], \"\", 1)\n",
    "    report_text = insert_string_at_position(report_text, replacement_term, annotation_to_replace.iloc[0][\"start_pos\"])\n",
    "    \n",
    "    with open(f\"{output_path}/{augmentation_file_name_no_ext}.txt\", \"w+\", encoding=\"utf8\") as aug_file:\n",
    "        aug_file.write(report_text)\n",
    "\n",
    "    # re-align annotations\n",
    "    result_annotations = pd.concat([annotations, annotation_to_replace], axis=0, ignore_index=True)\n",
    "    result_annotations = result_annotations.drop_duplicates(subset=[\"start_pos\", \"end_pos\", \"text\", \"ann_type\", \"entity_type\"], keep=False)\n",
    "\n",
    "    if not result_annotations.empty:\n",
    "        result_annotations[\"start_pos\"] = result_annotations.apply(lambda row: get_start_pos(row, report_text), axis=1)\n",
    "        result_annotations[\"end_pos\"] = result_annotations.apply(get_end_pos, axis=1)\n",
    "\n",
    "    # add the replacement annotation\n",
    "    annotation_to_replace[\"end_pos\"] = annotation_to_replace[\"start_pos\"] + len(replacement_term)\n",
    "    annotation_to_replace[\"text\"] = replacement_term\n",
    "    result_annotations = pd.concat([result_annotations, annotation_to_replace], axis=0, ignore_index=True)\n",
    "    \n",
    "    result_annotations['entity_type_with_positions'] = result_annotations.apply(lambda row: f'{row[\"entity_type\"]} {row[\"start_pos\"]} {row[\"end_pos\"]}', axis=1)\n",
    "    result_annotations.drop(columns=['start_pos', 'end_pos', 'entity_type', 'synonyms', 'has_multiple_synonyms'], inplace=True) \n",
    "    result_annotations.to_csv(f\"{output_path}/{augmentation_file_name_no_ext}.ann\", encoding=\"utf8\", sep=\"\\t\", columns=['ann_type', 'entity_type_with_positions', 'text'], index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "e4d75ace-653d-450a-886c-3402bc743cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unchanged_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
